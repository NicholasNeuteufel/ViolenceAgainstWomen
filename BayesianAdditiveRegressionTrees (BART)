#Using Bayesian additive regression trees (BART) to predict prevalence rates of violence 
#against women worldwide. Original BART paper is Chipman, George, & McCulloch (2008).
#URL: http://www-old.newton.ac.uk/preprints/NI09002.pdf
#I will use the R package bartMachine by Kapelner & Bleich (2013?)
#URL: http://cran.r-project.org/web/packages/bartMachine/vignettes/bartMachine_vignette.pdf

library(bartMachine)

#Read data with v1.2 variables
#Link: https://www.dropbox.com/s/yfmufbni32ox25b/FilledWithDroughtAndCorruption.csv?dl=0
data <- read.csv(file="FilledWithDroughtAndCorruption.csv", h=T, sep=",")

#Set seed for reproducible results
set.seed(12)

attach(data)

#Make dataframes and vector for bartMachine. #1 is without GPS data; #2 is with.
predictors1 <- cbind(GDP, Health.Spending, AgEmploy, Female.No.Education, LaborForceParticipationRatio, drought, corrupt)
predictors2 <- cbind(GDP, Health.Spending, AgEmploy, Female.No.Education, LaborForceParticipationRatio, drought, corrupt, lat, lon)
predictors1 <- data.frame(predictors1)
predictors2 <- data.frame(predictors2)
response <- IPV.Either.Life
y <- response #Must be named "y" for bartMachine
both1 <- cbind(predictors1, y)
both2 <- cbind(predictors2, y)

#BART model fitting is quite an intensive process. I give R more processing room to work with. 
#My laptop is quad-core (2.60 GHz) with 8.0 GB of RAM; I am running Windows 8.1.
#Even for my workhorse laptop, it takes at least 15 minutes.
set_bart_machine_memory(5000)
#Java initialized with 4.66GB maximum memory.
set_bart_machine_num_cores(4)
#bartMachine now using 4 cores.

#Create the BART models, selecting the parameters using cross-validation. 
cv.model1 <- bartMachineCV(X=predictors1, y=response, Xy=both1, num_tree_cv=c(50,100,200,500,1000), k_cvs=c(2,3,5,10,20), nu_q_cvs=list(c(3,0.9), c(3,0.99), c(10,0.75)), k_folds=5, verbose=T)
#>100 lines of a bunch of crap THEN
#bartMachine initializing with 500 trees...
#Now building bartMachine for regression ...
#evaluating in sample data...done

#Not going to do verbose=T for model2
cv.model2 <- bartMachineCV(X=predictors2, y=response, Xy=both2, num_tree_cv=c(50,100,200,500,1000), k_cvs=c(2,3,5,10,20), nu_q_cvs=list(c(3,0.9), c(3,0.99), c(10,0.75)), k_folds=5, verbose=F)

#Learn about how good the models are

summary(cv.model1)
#bartMachine v1.0.2 for regression
#
#training data n = 73 and p = 7 
#built in 4 secs on 4 cores, 500 trees, 250 burn-in and 1000 post. samples
#
#sigsq est for y beforehand: 225.778 
#avg sigsq estimate after burn-in: 236.3503 
#
#in-sample statistics:
# L1 = 897.67 
# L2 = 17116.71 
# rmse = 15.31 
# Pseudo-Rsq = 3e-04
#p-val for shapiro-wilk test of normality of residuals: 0.02665 
#p-val for zero-mean noise: 0.29372 

summary(cv.model2)
#bartMachine v1.0.2 for regression
#
#training data n = 73 and p = 9 
#built in 0.4 secs on 4 cores, 50 trees, 250 burn-in and 1000 post. samples
#
#sigsq est for y beforehand: 173.695 
#avg sigsq estimate after burn-in: 156.30298 
#
#in-sample statistics:
# L1 = 623.02 
# L2 = 8522.04 
# rmse = 10.8 
# Pseudo-Rsq = 0.5023
#p-val for shapiro-wilk test of normality of residuals: 0.04185 
#p-val for zero-mean noise: 0.95684

#Differences in pseudo-Rsq and RMSE are ENORMOUS; geography makes the difference here.
#Since BART models use up so much RAM, I'll delete the non-gegraphic cv.model1
destroy_bart_machine(cv.model1)

#See which variables matter most
png("bartVarImportance.png")
investigate_var_importance(cv.model2, num_replicates_for_avg=100)
#....................................................................................................
dev.off()
#windows 
#      2 
dev.off()
#Saved as PNG; link: https://www.dropbox.com/s/jrzc4hrsvlblvzx/bartVarImportance.png?dl=0

#See RMSE by number of trees
rmse_by_num_trees(cv.model2, tree_list=c(5, seq(10, 50, 10), 100, 150, 200, 500, 750, 1000), in_sample=F, plot=T, holdout_pctg=0.2, num_replicates=5)

#Using default paramters (non-CV BART model)
non.cv.model <- bartMachine(X=predictors2, y= response, Xy=both2, num_trees=1000, num_burn_in=250, num_iterations_after_burn_in=1000, alpha=0.95, beta=2, k=2, q=0.9, nu=3, run_in_sample=T, s_sq_y="mse")
#bartMachine initializing with 1000 trees...
#Now building bartMachine for regression ...
#evaluating in sample data...done

summary(non.cv.model)

#bartMachine v1.0.2 for regression
#
#training data n = 73 and p = 9 
#built in 7.9 secs on 4 cores, 1000 trees, 250 burn-in and 1000 post. samples
#
#sigsq est for y beforehand: 173.695 
#avg sigsq estimate after burn-in: 155.75492 
#
#in-sample statistics:
# L1 = 617.93 
# L2 = 8430.22 
# rmse = 10.75 
# Pseudo-Rsq = 0.5076
#p-val for shapiro-wilk test of normality of residuals: 0.02433 
#p-val for zero-mean noise: 0.94551 

png("bartVarImportanceNonCV.png")
investigate_var_importance(non.cv.model, num_replicates_for_avg=100)
#....................................................................................................
dev.off()

#The best thing I've seen a long time

non.cv.model1 <- bartMachine(X=predictors2, y=response, Xy=both2, num_trees=1000, num_burn_in=250, num_iterations_after_burn_in=1000, alpha=0.95, beta=2, k=1, q=0.9, nu=3, run_in_sample=T, s_sq_y="mse")
#bartMachine initializing with 1000 trees...
#Now building bartMachine for regression ...
#evaluating in sample data...done

summary(non.cv.model1)
#bartMachine v1.0.2 for regression
#
#training data n = 73 and p = 9 
#built in 11.5 secs on 4 cores, 1000 trees, 250 burn-in and 1000 post. samples
#
#sigsq est for y beforehand: 173.695 
#avg sigsq estimate after burn-in: 86.21823 
#
#in-sample statistics:
# L1 = 323.59 
# L2 = 2307.34 
# rmse = 5.62 
# Pseudo-Rsq = 0.8652
#p-val for shapiro-wilk test of normality of residuals: 0.0652 
#p-val for zero-mean noise: 0.982 
